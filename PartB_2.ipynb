{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# CS4001/4042 Assignment 1, Part B, Q2\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-widedeep in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: imutils in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (0.5.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (4.3.2)\n",
      "Requirement already satisfied: pandas>=1.3.5 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (2.0.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (4.62.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (13.0.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (4.8.1.78)\n",
      "Requirement already satisfied: fastparquet>=0.8.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (2023.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (0.15.2)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (0.11.4)\n",
      "Requirement already satisfied: spacy in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (3.6.1)\n",
      "Requirement already satisfied: einops in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (0.6.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (1.10.1)\n",
      "Requirement already satisfied: torch in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (2.0.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pytorch-widedeep) (1.12.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (21.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2.7.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2021.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.3.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from gensim->pytorch-widedeep) (6.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from packaging->fastparquet>=0.8.1->pytorch-widedeep) (3.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (1.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\keerthana\\appdata\\roaming\\python\\python39\\site-packages (from spacy->pytorch-widedeep) (1.10.13)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (2.26.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (58.0.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (0.10.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (8.1.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (2.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from spacy->pytorch-widedeep) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from tqdm->pytorch-widedeep) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy->pytorch-widedeep) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from jinja2->spacy->pytorch-widedeep) (1.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from torch->pytorch-widedeep) (3.3.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from torch->pytorch-widedeep) (2.6.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from torch->pytorch-widedeep) (1.9)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from sympy->torch->pytorch-widedeep) (1.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from torchvision->pytorch-widedeep) (8.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# Divide the dataset into train and test sets\n",
    "train_data = df[df['year'] <= 2020]\n",
    "test_data = df[df['year'] >= 2021]\n",
    "\n",
    "# Define target variable and features\n",
    "target = ['resale_price']\n",
    "continuous_cols = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "cat_embed_cols = ['month', 'town', 'flat_model_type', 'storey_range']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keerthana\\anaconda3\\New folder\\lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:334: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|█████████████████████████████| 1366/1366 [00:12<00:00, 111.28it/s, loss=2.76e+5, metrics={'r2': -3.2408}]\n",
      "epoch 2: 100%|██████████████████████████████| 1366/1366 [00:11<00:00, 117.89it/s, loss=1.24e+5, metrics={'r2': 0.3058}]\n",
      "epoch 3: 100%|██████████████████████████████| 1366/1366 [00:10<00:00, 125.53it/s, loss=1.19e+5, metrics={'r2': 0.3494}]\n",
      "epoch 4: 100%|██████████████████████████████| 1366/1366 [00:11<00:00, 119.93it/s, loss=1.18e+5, metrics={'r2': 0.3588}]\n",
      "epoch 5: 100%|██████████████████████████████| 1366/1366 [00:12<00:00, 105.49it/s, loss=1.18e+5, metrics={'r2': 0.3594}]\n",
      "epoch 6: 100%|███████████████████████████████| 1366/1366 [00:12<00:00, 107.47it/s, loss=1.18e+5, metrics={'r2': 0.363}]\n",
      "epoch 7: 100%|██████████████████████████████| 1366/1366 [00:11<00:00, 114.18it/s, loss=1.18e+5, metrics={'r2': 0.3656}]\n",
      "epoch 8: 100%|██████████████████████████████| 1366/1366 [00:13<00:00, 101.00it/s, loss=1.17e+5, metrics={'r2': 0.3667}]\n",
      "epoch 9: 100%|██████████████████████████████| 1366/1366 [00:12<00:00, 111.35it/s, loss=1.17e+5, metrics={'r2': 0.3709}]\n",
      "epoch 10: 100%|█████████████████████████████| 1366/1366 [00:13<00:00, 104.32it/s, loss=1.16e+5, metrics={'r2': 0.3782}]\n",
      "epoch 11: 100%|█████████████████████████████| 1366/1366 [00:11<00:00, 117.45it/s, loss=1.16e+5, metrics={'r2': 0.3832}]\n",
      "epoch 12: 100%|█████████████████████████████| 1366/1366 [00:12<00:00, 110.95it/s, loss=1.16e+5, metrics={'r2': 0.3842}]\n",
      "epoch 13: 100%|█████████████████████████████| 1366/1366 [00:12<00:00, 113.09it/s, loss=1.15e+5, metrics={'r2': 0.3857}]\n",
      "epoch 14: 100%|█████████████████████████████| 1366/1366 [00:12<00:00, 113.49it/s, loss=1.15e+5, metrics={'r2': 0.3895}]\n",
      "epoch 15: 100%|██████████████████████████████| 1366/1366 [00:16<00:00, 85.37it/s, loss=1.15e+5, metrics={'r2': 0.3927}]\n",
      "epoch 16: 100%|█████████████████████████████| 1366/1366 [00:12<00:00, 109.20it/s, loss=1.14e+5, metrics={'r2': 0.3952}]\n",
      "epoch 17: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 129.05it/s, loss=1.14e+5, metrics={'r2': 0.3953}]\n",
      "epoch 18: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 131.42it/s, loss=1.14e+5, metrics={'r2': 0.3992}]\n",
      "epoch 19: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 131.61it/s, loss=1.14e+5, metrics={'r2': 0.4017}]\n",
      "epoch 20: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 144.23it/s, loss=1.13e+5, metrics={'r2': 0.4057}]\n",
      "epoch 21: 100%|█████████████████████████████| 1366/1366 [00:08<00:00, 166.22it/s, loss=1.13e+5, metrics={'r2': 0.4056}]\n",
      "epoch 22: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.46it/s, loss=1.13e+5, metrics={'r2': 0.4122}]\n",
      "epoch 23: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 139.60it/s, loss=1.12e+5, metrics={'r2': 0.4186}]\n",
      "epoch 24: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 139.29it/s, loss=1.12e+5, metrics={'r2': 0.4165}]\n",
      "epoch 25: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 135.21it/s, loss=1.11e+5, metrics={'r2': 0.4255}]\n",
      "epoch 26: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 139.93it/s, loss=1.11e+5, metrics={'r2': 0.4243}]\n",
      "epoch 27: 100%|███████████████████████████████| 1366/1366 [00:09<00:00, 151.45it/s, loss=1.1e+5, metrics={'r2': 0.433}]\n",
      "epoch 28: 100%|███████████████████████████████| 1366/1366 [00:09<00:00, 149.85it/s, loss=1.1e+5, metrics={'r2': 0.434}]\n",
      "epoch 29: 100%|██████████████████████████████| 1366/1366 [00:10<00:00, 133.11it/s, loss=1.1e+5, metrics={'r2': 0.4388}]\n",
      "epoch 30: 100%|██████████████████████████████| 1366/1366 [00:09<00:00, 145.71it/s, loss=1.1e+5, metrics={'r2': 0.4393}]\n",
      "epoch 31: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 141.04it/s, loss=1.09e+5, metrics={'r2': 0.4421}]\n",
      "epoch 32: 100%|██████████████████████████████| 1366/1366 [00:09<00:00, 144.56it/s, loss=1.09e+5, metrics={'r2': 0.445}]\n",
      "epoch 33: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 144.86it/s, loss=1.09e+5, metrics={'r2': 0.4486}]\n",
      "epoch 34: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 151.01it/s, loss=1.08e+5, metrics={'r2': 0.4507}]\n",
      "epoch 35: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.48it/s, loss=1.08e+5, metrics={'r2': 0.4568}]\n",
      "epoch 36: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 148.60it/s, loss=1.08e+5, metrics={'r2': 0.4576}]\n",
      "epoch 37: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 138.65it/s, loss=1.07e+5, metrics={'r2': 0.4612}]\n",
      "epoch 38: 100%|██████████████████████████████| 1366/1366 [00:10<00:00, 131.55it/s, loss=1.07e+5, metrics={'r2': 0.465}]\n",
      "epoch 39: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 126.53it/s, loss=1.07e+5, metrics={'r2': 0.4677}]\n",
      "epoch 40: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 132.59it/s, loss=1.06e+5, metrics={'r2': 0.4736}]\n",
      "epoch 41: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 138.58it/s, loss=1.06e+5, metrics={'r2': 0.4741}]\n",
      "epoch 42: 100%|██████████████████████████████| 1366/1366 [00:09<00:00, 142.17it/s, loss=1.06e+5, metrics={'r2': 0.476}]\n",
      "epoch 43: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 143.50it/s, loss=1.05e+5, metrics={'r2': 0.4795}]\n",
      "epoch 44: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 133.98it/s, loss=1.05e+5, metrics={'r2': 0.4807}]\n",
      "epoch 45: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 132.74it/s, loss=1.05e+5, metrics={'r2': 0.4825}]\n",
      "epoch 46: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 133.22it/s, loss=1.05e+5, metrics={'r2': 0.4853}]\n",
      "epoch 47: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 129.83it/s, loss=1.04e+5, metrics={'r2': 0.4884}]\n",
      "epoch 48: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 151.58it/s, loss=1.04e+5, metrics={'r2': 0.4893}]\n",
      "epoch 49: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.05it/s, loss=1.04e+5, metrics={'r2': 0.4895}]\n",
      "epoch 50: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 151.76it/s, loss=1.04e+5, metrics={'r2': 0.4896}]\n",
      "epoch 51: 100%|██████████████████████████████| 1366/1366 [00:09<00:00, 150.84it/s, loss=1.04e+5, metrics={'r2': 0.493}]\n",
      "epoch 52: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 151.05it/s, loss=1.04e+5, metrics={'r2': 0.4947}]\n",
      "epoch 53: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 151.30it/s, loss=1.04e+5, metrics={'r2': 0.4927}]\n",
      "epoch 54: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.95it/s, loss=1.04e+5, metrics={'r2': 0.4949}]\n",
      "epoch 55: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 150.51it/s, loss=1.03e+5, metrics={'r2': 0.4989}]\n",
      "epoch 56: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 148.65it/s, loss=1.03e+5, metrics={'r2': 0.4982}]\n",
      "epoch 57: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 147.92it/s, loss=1.03e+5, metrics={'r2': 0.4987}]\n",
      "epoch 58: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.78it/s, loss=1.03e+5, metrics={'r2': 0.5005}]\n",
      "epoch 59: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.55it/s, loss=1.03e+5, metrics={'r2': 0.4995}]\n",
      "epoch 60: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 150.53it/s, loss=1.03e+5, metrics={'r2': 0.4981}]\n",
      "epoch 61: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.90it/s, loss=1.03e+5, metrics={'r2': 0.5022}]\n",
      "epoch 62: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 150.00it/s, loss=1.03e+5, metrics={'r2': 0.4987}]\n",
      "epoch 63: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.53it/s, loss=1.03e+5, metrics={'r2': 0.5006}]\n",
      "epoch 64: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.08it/s, loss=1.03e+5, metrics={'r2': 0.5032}]\n",
      "epoch 65: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.51it/s, loss=1.03e+5, metrics={'r2': 0.5014}]\n",
      "epoch 66: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 150.00it/s, loss=1.03e+5, metrics={'r2': 0.5028}]\n",
      "epoch 67: 100%|█████████████████████████████| 1366/1366 [00:08<00:00, 161.26it/s, loss=1.03e+5, metrics={'r2': 0.5028}]\n",
      "epoch 68: 100%|█████████████████████████████| 1366/1366 [00:08<00:00, 156.70it/s, loss=1.03e+5, metrics={'r2': 0.5039}]\n",
      "epoch 69: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 138.32it/s, loss=1.03e+5, metrics={'r2': 0.5056}]\n",
      "epoch 70: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.85it/s, loss=1.03e+5, metrics={'r2': 0.5015}]\n",
      "epoch 71: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 140.29it/s, loss=1.03e+5, metrics={'r2': 0.5042}]\n",
      "epoch 72: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 142.97it/s, loss=1.03e+5, metrics={'r2': 0.5056}]\n",
      "epoch 73: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.58it/s, loss=1.03e+5, metrics={'r2': 0.5065}]\n",
      "epoch 74: 100%|█████████████████████████████| 1366/1366 [00:08<00:00, 153.29it/s, loss=1.03e+5, metrics={'r2': 0.5042}]\n",
      "epoch 75: 100%|█████████████████████████████| 1366/1366 [00:08<00:00, 153.29it/s, loss=1.03e+5, metrics={'r2': 0.5055}]\n",
      "epoch 76: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 148.08it/s, loss=1.03e+5, metrics={'r2': 0.5037}]\n",
      "epoch 77: 100%|█████████████████████████████| 1366/1366 [00:08<00:00, 152.03it/s, loss=1.03e+5, metrics={'r2': 0.5036}]\n",
      "epoch 78: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 150.04it/s, loss=1.02e+5, metrics={'r2': 0.5085}]\n",
      "epoch 79: 100%|██████████████████████████████| 1366/1366 [00:09<00:00, 147.86it/s, loss=1.02e+5, metrics={'r2': 0.508}]\n",
      "epoch 80: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 140.94it/s, loss=1.02e+5, metrics={'r2': 0.5075}]\n",
      "epoch 81: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 151.65it/s, loss=1.02e+5, metrics={'r2': 0.5069}]\n",
      "epoch 82: 100%|█████████████████████████████| 1366/1366 [00:08<00:00, 152.40it/s, loss=1.02e+5, metrics={'r2': 0.5079}]\n",
      "epoch 83: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 140.94it/s, loss=1.03e+5, metrics={'r2': 0.5039}]\n",
      "epoch 84: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 149.46it/s, loss=1.02e+5, metrics={'r2': 0.5072}]\n",
      "epoch 85: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 148.84it/s, loss=1.03e+5, metrics={'r2': 0.5058}]\n",
      "epoch 86: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 150.24it/s, loss=1.02e+5, metrics={'r2': 0.5083}]\n",
      "epoch 87: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 146.31it/s, loss=1.02e+5, metrics={'r2': 0.5065}]\n",
      "epoch 88: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 148.36it/s, loss=1.02e+5, metrics={'r2': 0.5077}]\n",
      "epoch 89: 100%|██████████████████████████████| 1366/1366 [00:10<00:00, 129.98it/s, loss=1.02e+5, metrics={'r2': 0.509}]\n",
      "epoch 90: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 124.99it/s, loss=1.02e+5, metrics={'r2': 0.5095}]\n",
      "epoch 91: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 132.46it/s, loss=1.02e+5, metrics={'r2': 0.5075}]\n",
      "epoch 92: 100%|█████████████████████████████| 1366/1366 [00:09<00:00, 142.78it/s, loss=1.02e+5, metrics={'r2': 0.5092}]\n",
      "epoch 93: 100%|█████████████████████████████| 1366/1366 [00:11<00:00, 119.50it/s, loss=1.02e+5, metrics={'r2': 0.5102}]\n",
      "epoch 94: 100%|██████████████████████████████| 1366/1366 [00:11<00:00, 121.65it/s, loss=1.02e+5, metrics={'r2': 0.511}]\n",
      "epoch 95: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 126.75it/s, loss=1.02e+5, metrics={'r2': 0.5113}]\n",
      "epoch 96: 100%|██████████████████████████████| 1366/1366 [00:11<00:00, 118.71it/s, loss=1.02e+5, metrics={'r2': 0.512}]\n",
      "epoch 97: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 124.73it/s, loss=1.02e+5, metrics={'r2': 0.5111}]\n",
      "epoch 98: 100%|█████████████████████████████| 1366/1366 [00:11<00:00, 118.94it/s, loss=1.02e+5, metrics={'r2': 0.5142}]\n",
      "epoch 99: 100%|█████████████████████████████| 1366/1366 [00:10<00:00, 133.14it/s, loss=1.02e+5, metrics={'r2': 0.5155}]\n",
      "epoch 100: 100%|████████████████████████████| 1366/1366 [00:11<00:00, 120.51it/s, loss=1.02e+5, metrics={'r2': 0.5128}]\n"
     ]
    }
   ],
   "source": [
    "tab_preprocessor = TabPreprocessor(\n",
    "    continuous_cols=continuous_cols,\n",
    "    categorical_cols=cat_embed_cols\n",
    ")\n",
    "X_train_tab = tab_preprocessor.fit_transform(train_data)\n",
    "X_test_tab = tab_preprocessor.transform(test_data)\n",
    "\n",
    "# Define the TabMlp model\n",
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    continuous_cols=continuous_cols,\n",
    "    mlp_hidden_dims=[200, 100]    \n",
    ")\n",
    "\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "\n",
    "# Create a Trainer for the model\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    objective=\"rmse\",\n",
    "    metrics=[R2Score()],\n",
    "    verbose=1,\n",
    "    seed=42,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Define the target variable for training\n",
    "target_train = train_data['resale_price'].values\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(X_tab=X_train_tab, target=target_train, n_epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|████████████████████████████████████████████████████████████████████| 1128/1128 [00:03<00:00, 341.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 144876.06406611577\n",
      "Test R2 Score: 0.2666829079552636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the trained model\n",
    "target_test = test_data['resale_price'].values\n",
    "predictions = trainer.predict(X_tab=X_test_tab)\n",
    "\n",
    "# Calculate RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = mean_squared_error(target_test, predictions, squared=False)\n",
    "\n",
    "# Calculate R2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(target_test, predictions)\n",
    "\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"Test R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
