{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDDK5s1_mFRg"
   },
   "source": [
    "CS4001/4042 Assignment 1, Part B, Q3\n",
    "---\n",
    "\n",
    "Besides ensuring that your neural network performs well, it is important to be able to explain the model’s decision. **Captum** is a very handy library that helps you to do so for PyTorch models.\n",
    "\n",
    "Many model explainability algorithms for deep learning models are available in Captum. These algorithms are often used to generate an attribution score for each feature. Features with larger scores are more ‘important’ and some algorithms also provide information about directionality (i.e. a feature with very negative attribution scores means the larger the value of that feature, the lower the value of the output).\n",
    "\n",
    "In general, these algorithms can be grouped into two paradigms:\n",
    "- **perturbation based approaches** (e.g. Feature Ablation)\n",
    "- **gradient / backpropagation based approaches** (e.g. Saliency)\n",
    "\n",
    "The former adopts a brute-force approach of removing / permuting features one by one and does not scale up well. The latter depends on gradients and they can be computed relatively quickly. But unlike how backpropagation computes gradients with respect to weights, gradients here are computed **with respect to the input**. This gives us a sense of how much a change in the input affects the model’s outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7WFI5tMpqGc"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mRCFpMEd3w8W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: captum in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from captum) (3.4.3)\n",
      "Requirement already satisfied: torch>=1.6 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from captum) (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from captum) (1.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from torch>=1.6->captum) (3.3.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from torch>=1.6->captum) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from torch>=1.6->captum) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from torch>=1.6->captum) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from torch>=1.6->captum) (1.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from jinja2->torch>=1.6->captum) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from matplotlib->captum) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from matplotlib->captum) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from matplotlib->captum) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from matplotlib->captum) (8.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from cycler>=0.10->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\keerthana\\anaconda3\\new folder\\lib\\site-packages (from sympy->torch>=1.6->captum) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "utC2haR03sQY"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from captum.attr import Saliency, InputXGradient, IntegratedGradients, GradientShap, FeatureAblation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUU-C3eRmWeE"
   },
   "source": [
    "> First, load the dataset following the splits in Question B1. To keep things simple, we will **limit our analysis to numeric / continuous features only**. Drop all categorical features from the dataframes. Do not standardise the numerical features for now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FNtpumjamL1N"
   },
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "train_data = df[df['year'] <= 2019]\n",
    "val_data = df[df['year'] == 2020]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "target = ['resale_price']\n",
    "\n",
    "\n",
    "numeric_features = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', \n",
    "                    'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "\n",
    "train_data_numeric = train_data[numeric_features]\n",
    "val_data_numeric = val_data[numeric_features]\n",
    "test_data_numeric = test_data[numeric_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4L7QdqLmX2s"
   },
   "source": [
    "> Follow this tutorial to generate the plot from various model explainability algorithms (https://captum.ai/tutorials/House_Prices_Regression_Interpret).\n",
    "Specifically, make the following changes:\n",
    "- Use a feedforward neural network with 3 hidden layers, each having 5 neurons. Train using Adam optimiser with learning rate of 0.001.\n",
    "- Use Saliency, Input x Gradients, Integrated Gradients, GradientSHAP, Feature Ablation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VGIWUq9Fmct8"
   },
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "X_train = torch.Tensor(train_data_numeric.values)\n",
    "y_train = torch.Tensor(train_data[target].values)\n",
    "\n",
    "batch_size = 1024\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "size_hidden1 = 100\n",
    "size_hidden2 = 50\n",
    "size_hidden3 = 10\n",
    "size_hidden4 = 1\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden1 = nn.Linear(6, 5)\n",
    "        self.hidden2 = nn.Linear(5, 5)\n",
    "        self.hidden3 = nn.Linear(5, 5)\n",
    "        self.output = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = torch.relu(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.train()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Train the model\n",
    "def train(model_inp, num_epochs = num_epochs):\n",
    "    optimizer = torch.optim.Adam(model_standardized.parameters(), lr=0.001)\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_iter:\n",
    "            outputs = model_inp(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        if epoch % 20 == 0:    \n",
    "            print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %\n",
    "                  (epoch + 1, num_epochs, running_loss))\n",
    "        running_loss = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keerthana\\anaconda3\\New folder\\lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saliency= tensor([[0.0013, 0.0003, 0.0010, 0.0009, 0.0008, 0.0027],\n",
      "        [0.0013, 0.0003, 0.0010, 0.0009, 0.0008, 0.0027],\n",
      "        [0.0013, 0.0003, 0.0010, 0.0009, 0.0008, 0.0027],\n",
      "        ...,\n",
      "        [0.0013, 0.0003, 0.0010, 0.0009, 0.0008, 0.0027],\n",
      "        [0.0013, 0.0003, 0.0010, 0.0009, 0.0008, 0.0027],\n",
      "        [0.0013, 0.0003, 0.0010, 0.0009, 0.0008, 0.0027]])\n",
      "Input x Gradients= tensor([[ 1.6157e-03,  2.8778e-03,  1.6520e-05,  2.2612e-06, -5.1981e-02,\n",
      "          1.2159e-01],\n",
      "        [ 1.6157e-03,  2.8778e-03,  1.6520e-05,  2.2612e-06, -5.1981e-02,\n",
      "          1.2159e-01],\n",
      "        [ 1.1197e-03,  2.4091e-03,  1.6520e-05,  5.7417e-06, -4.7858e-02,\n",
      "          1.8373e-01],\n",
      "        ...,\n",
      "        [ 6.9358e-04,  3.1515e-03,  1.6520e-05,  2.2612e-06, -4.6911e-02,\n",
      "          1.8373e-01],\n",
      "        [ 9.7030e-04,  2.6001e-03,  1.6520e-05,  5.7417e-06, -4.6100e-02,\n",
      "          1.8103e-01],\n",
      "        [ 8.8964e-04,  2.8909e-03,  1.6520e-05,  5.7417e-06, -4.6438e-02,\n",
      "          1.8373e-01]], grad_fn=<MulBackward0>)\n",
      "Integrated Gradients= tensor([[ 1.5536e-03,  2.5304e-03,  1.5465e-05,  2.3107e-06, -5.0439e-02,\n",
      "          1.1593e-01],\n",
      "        [ 1.5536e-03,  2.5304e-03,  1.5465e-05,  2.3107e-06, -5.0439e-02,\n",
      "          1.1593e-01],\n",
      "        [ 1.0517e-03,  1.8905e-03,  1.5346e-05,  5.6166e-06, -4.5235e-02,\n",
      "          1.7781e-01],\n",
      "        ...,\n",
      "        [ 6.4865e-04,  2.4136e-03,  1.5286e-05,  2.2012e-06, -4.4131e-02,\n",
      "          1.7794e-01],\n",
      "        [ 9.1139e-04,  2.0404e-03,  1.5346e-05,  5.6166e-06, -4.3573e-02,\n",
      "          1.7520e-01],\n",
      "        [ 8.3201e-04,  2.2139e-03,  1.5286e-05,  5.5894e-06, -4.3686e-02,\n",
      "          1.7794e-01]], dtype=torch.float64)\n",
      "GradientSHAP= tensor([[ 1.6157e-03,  2.8778e-03,  1.6520e-05,  2.2612e-06, -5.1981e-02,\n",
      "          1.2159e-01],\n",
      "        [ 1.9186e-03,  3.9227e-03,  1.9529e-05,  2.7034e-06, -6.1971e-02,\n",
      "          1.3758e-01],\n",
      "        [ 1.3297e-03,  3.2839e-03,  1.9529e-05,  6.8645e-06, -5.7055e-02,\n",
      "          2.0789e-01],\n",
      "        ...,\n",
      "        [ 6.9358e-04,  3.1515e-03,  1.6520e-05,  2.2612e-06, -4.6911e-02,\n",
      "          1.8373e-01],\n",
      "        [ 1.1522e-03,  3.5442e-03,  1.9529e-05,  6.8645e-06, -5.4960e-02,\n",
      "          2.0484e-01],\n",
      "        [ 1.0190e-03,  3.3739e-03,  1.8911e-05,  6.5822e-06, -5.3216e-02,\n",
      "          2.0917e-01]])\n",
      "Feature Ablation= tensor([[ 1.6157e-03,  2.8778e-03,  1.6540e-05,  2.2948e-06, -1.4047e-03,\n",
      "          3.1121e-02],\n",
      "        [ 1.6157e-03,  2.8778e-03,  1.6540e-05,  2.2948e-06, -1.4047e-03,\n",
      "          3.1121e-02],\n",
      "        [ 1.1197e-03,  1.1571e-02,  1.6570e-05,  5.7220e-06, -5.7118e-03,\n",
      "          9.9259e-02],\n",
      "        ...,\n",
      "        [ 6.9368e-04,  1.3211e-02,  1.6570e-05,  2.3246e-06,  3.6678e-03,\n",
      "          9.9141e-02],\n",
      "        [ 9.7024e-04,  1.2263e-02,  1.6451e-05,  5.7220e-06, -1.8727e-03,\n",
      "          9.8531e-02],\n",
      "        [ 8.8966e-04,  1.2984e-02,  1.6451e-05,  5.7220e-06,  1.3316e-03,\n",
      "          1.0012e-01]])\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.Tensor(test_data_numeric.values)\n",
    "\n",
    "# Saliency\n",
    "saliency = Saliency(model)\n",
    "attributions_saliency = saliency.attribute(X_test[:1000], target=0)\n",
    "print('Saliency=', attributions_saliency)\n",
    "\n",
    "\n",
    "# Input x Gradients\n",
    "input_x_grad = InputXGradient(model)\n",
    "attributions_input_x_grad = input_x_grad.attribute(X_test[:1000], target=0)\n",
    "print('Input x Gradients=', attributions_input_x_grad)\n",
    "\n",
    "\n",
    "# Integrated Gradients\n",
    "integrated_grad = IntegratedGradients(model)\n",
    "attributions_integrated_grad = integrated_grad.attribute(X_test[:1000], target=0)\n",
    "print('Integrated Gradients=', attributions_integrated_grad)\n",
    "\n",
    "\n",
    "# GradientSHAP\n",
    "gradient_shap = GradientShap(model)\n",
    "baselines = torch.zeros(X_test[:1000].shape)  \n",
    "attributions_gradient_shap = gradient_shap.attribute(X_test[:1000], baselines=baselines, target=0)\n",
    "print('GradientSHAP=', attributions_gradient_shap)\n",
    "\n",
    "# Feature Ablation\n",
    "feature_ablation = FeatureAblation(model)\n",
    "attributions_feature_ablation = feature_ablation.attribute(X_test[:1000], target=0)\n",
    "print('Feature Ablation=', attributions_feature_ablation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DexR-OzAmd26"
   },
   "source": [
    "> Train a separate model with the same configuration but now standardise the features via **StandardScaler** (fit to training set, then transform all). State your observations with respect to GradientShap and explain why it has occurred.\n",
    "(Hint: Many gradient-based approaches depend on a baseline, which is an important choice to be made. Check the default baseline settings carefully.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yzRk02TnmgyB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 13859052010340352.0000\n",
      "Epoch [21/100], Loss: 13859048789114880.0000\n",
      "Epoch [41/100], Loss: 13859046641631232.0000\n",
      "Epoch [61/100], Loss: 13859044494147584.0000\n",
      "Epoch [81/100], Loss: 13859039125438464.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keerthana\\anaconda3\\New folder\\lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saliency= tensor([[0.0063, 0.0442, 0.0438, 0.0468, 0.0240, 0.0284],\n",
      "        [0.0063, 0.0442, 0.0438, 0.0468, 0.0240, 0.0284],\n",
      "        [0.0055, 0.0248, 0.0256, 0.0335, 0.0204, 0.0219],\n",
      "        ...,\n",
      "        [0.0015, 0.0544, 0.0440, 0.0394, 0.0245, 0.0282],\n",
      "        [0.0015, 0.0544, 0.0440, 0.0394, 0.0245, 0.0282],\n",
      "        [0.0015, 0.0544, 0.0440, 0.0394, 0.0245, 0.0282]])\n",
      "Input x Gradients= tensor([[-0.0065,  0.0301, -0.0063, -0.0096,  0.0201,  0.0618],\n",
      "        [-0.0065,  0.0301, -0.0063, -0.0096,  0.0201,  0.0618],\n",
      "        [-0.0010,  0.0249, -0.0037, -0.0009,  0.0254,  0.0268],\n",
      "        ...,\n",
      "        [-0.0008,  0.0269, -0.0063, -0.0081,  0.0328,  0.0346],\n",
      "        [-0.0001,  0.0474, -0.0063, -0.0010,  0.0348,  0.0358],\n",
      "        [-0.0003,  0.0366, -0.0063, -0.0010,  0.0340,  0.0346]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Integrated Gradients= tensor([[-0.0077,  0.0295, -0.0066, -0.0055,  0.0108,  0.0180],\n",
      "        [-0.0077,  0.0295, -0.0066, -0.0055,  0.0108,  0.0180],\n",
      "        [-0.0016,  0.0505, -0.0079, -0.0006,  0.0111, -0.0002],\n",
      "        ...,\n",
      "        [ 0.0049,  0.0223, -0.0073, -0.0031,  0.0076, -0.0063],\n",
      "        [ 0.0006,  0.0442, -0.0078, -0.0005,  0.0121, -0.0009],\n",
      "        [ 0.0017,  0.0331, -0.0076, -0.0005,  0.0112, -0.0016]],\n",
      "       dtype=torch.float64)\n",
      "GradientSHAP= tensor([[-0.0107,  0.0272, -0.0069, -0.0030,  0.0041, -0.0128],\n",
      "        [-0.0086,  0.0287, -0.0066, -0.0063,  0.0121,  0.0245],\n",
      "        [-0.0011,  0.0515, -0.0074, -0.0009,  0.0215,  0.0177],\n",
      "        ...,\n",
      "        [ 0.0049,  0.0208, -0.0069, -0.0027,  0.0066, -0.0072],\n",
      "        [ 0.0003,  0.0420, -0.0066, -0.0007,  0.0209,  0.0141],\n",
      "        [ 0.0019,  0.0282, -0.0069, -0.0004,  0.0069, -0.0072]])\n",
      "Feature Ablation= tensor([[-0.0047,  0.0308, -0.0063, -0.0096,  0.0201,  0.0561],\n",
      "        [-0.0047,  0.0308, -0.0063, -0.0096,  0.0201,  0.0561],\n",
      "        [-0.0010,  0.0523, -0.0037, -0.0009,  0.0297,  0.0206],\n",
      "        ...,\n",
      "        [-0.0008,  0.0269, -0.0063, -0.0081,  0.0328,  0.0138],\n",
      "        [-0.0001,  0.0474, -0.0054, -0.0010,  0.0348,  0.0246],\n",
      "        [-0.0003,  0.0366, -0.0063, -0.0010,  0.0340,  0.0242]])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(train_data_numeric)\n",
    "X_test_standardized = scaler.transform(test_data_numeric)\n",
    "\n",
    "X_train_standardized = torch.Tensor(X_train_standardized)\n",
    "X_test_standardized = torch.Tensor(X_test_standardized)\n",
    "y_train = torch.Tensor(train_data[target].values)\n",
    "\n",
    "model_standardized = NeuralNetwork()\n",
    "\n",
    "model_standardized.train()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model_standardized.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_standardized(X_train_standardized)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Saliency\n",
    "saliency_standardized = Saliency(model_standardized)\n",
    "attributions_saliency_standardized = saliency_standardized.attribute(X_test_standardized[:1000], target=0)\n",
    "print('Saliency=', attributions_saliency_standardized)\n",
    "                                                                     \n",
    "\n",
    "# Input x Gradients\n",
    "input_x_grad_standardized = InputXGradient(model_standardized)\n",
    "attributions_input_x_grad_standardized = input_x_grad_standardized.attribute(X_test_standardized[:1000], target=0)\n",
    "print('Input x Gradients=', attributions_input_x_grad_standardized)\n",
    "\n",
    "# Integrated Gradients\n",
    "integrated_grad_standardized = IntegratedGradients(model_standardized)\n",
    "attributions_integrated_grad_standardized = integrated_grad_standardized.attribute(X_test_standardized[:1000], target=0)\n",
    "print('Integrated Gradients=', attributions_integrated_grad_standardized)\n",
    "\n",
    "# GradientSHAP\n",
    "gradient_shap_standardized = GradientShap(model_standardized)\n",
    "baselines_standardized = torch.zeros(X_test_standardized[:1000].shape)  \n",
    "attributions_gradient_shap_standardized = gradient_shap_standardized.attribute(X_test_standardized[:1000], baselines=baselines_standardized, target=0)\n",
    "print('GradientSHAP=', attributions_gradient_shap_standardized)\n",
    "\n",
    "# Feature Ablation\n",
    "feature_ablation_standardized = FeatureAblation(model_standardized)\n",
    "attributions_feature_ablation_standardized = feature_ablation_standardized.attribute(X_test_standardized[:1000], target=0)\n",
    "print('Feature Ablation=', attributions_feature_ablation_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When standardized, GradientShap provides relatively coherent and interpretable attribution scores, indicating the importance of features for predicting the target variable. However, in the non-scaled version, the attribution scores provided by GradientShap are less interpretable and seem inconsistent. This discrepancy occurs because GradientShap relies on a baseline value (in this case, a tensor of zeros) to compute the attributions. When features are not scaled, the variation in the feature values can be substantial, making it difficult to attribute changes in the output solely to changes in a specific feature. This can result in less meaningful and less reliable attribution scores from GradientShap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX9iqK6SmhQ5"
   },
   "source": [
    "Read https://distill.pub/2020/attribution-baselines/ to build up your understanding of Integrated Gradients (IG). Reading the sections before the section on ‘Game Theory and Missingness’ will be sufficient. Keep in mind that this article mainly focuses on classification problems. You might find the following [descriptions](https://captum.ai/docs/attribution_algorithms) and [comparisons](https://captum.ai/docs/algorithms_comparison_matrix) in Captum useful as well.\n",
    "\n",
    "\n",
    "Then, answer the following questions in the context of our dataset:\n",
    "\n",
    "> Why did Saliency produce scores similar to IG?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67WqoEltmlRb"
   },
   "source": [
    "\\# TODO: \\<Enter your answer here\\> \n",
    "\n",
    "Saliency computes gradients with respect to the input features (input attribution), while IG is a method that approximates integrated gradients using a series of linear interpolations between a baseline and the input (input attribution as well).\n",
    "\n",
    "When standardized features are used, the feature values have been scaled to a similar range. This scaling makes the feature contributions to the model's output more consistent and less sensitive to the magnitude of the feature values. As a result, Saliency and IG produce similar attribution scores because they are both measuring how a change in each feature impacts the model's output, and the scaling aligns their attributions.\n",
    "\n",
    "In the non-standardized case, the attribution scores may differ because feature values have different scales, and the models' sensitivities to the individual features can vary significantly. This leads to different attribution patterns between Saliency and IG, where Saliency might be more affected by the magnitude of feature values while IG is less sensitive due to the integration step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYpfn3nCml1K"
   },
   "source": [
    "> Why did Input x Gradients give the same attribution scores as IG?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5OmMEdMmnP_"
   },
   "source": [
    "\\# TODO: \\<Enter your answer here\\>\n",
    "\n",
    "Input x Gradients is another gradient-based algorithm that computes the gradient of the model's output with respect to the input features (input attribution).\n",
    "\n",
    "When standardized features are used, both Input x Gradients and IG provide similar attribution scores because they both rely on gradient computations with respect to the input. Standardization helps ensure that the gradients are consistent in terms of feature magnitudes, leading to similar attribution patterns between the two methods.\n",
    "\n",
    "In the non-standardized case, the attribution scores may also differ because Input x Gradients is directly affected by the magnitude of the feature values, whereas IG mitigates this sensitivity by integrating the gradient across the entire feature space. Consequently, if the features have different scales, Input x Gradients can give higher importance to features with larger values, while IG normalizes these variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
